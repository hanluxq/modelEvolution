{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJpBQvu-GnKQ"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rc03n67z_JCB"
   },
   "source": [
    "# 微调分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zZH5n05AYRC"
   },
   "source": [
    "## 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37205,
     "status": "ok",
     "timestamp": 1682390217572,
     "user": {
      "displayName": "lu han",
      "userId": "00064302419450254877"
     },
     "user_tz": -480
    },
    "id": "P-SxG-EQmawW",
    "outputId": "6e758690-b683-46bc-8241-438f16196799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27014,
     "status": "ok",
     "timestamp": 1682391938396,
     "user": {
      "displayName": "lu han",
      "userId": "00064302419450254877"
     },
     "user_tz": -480
    },
    "id": "BaaX2vPxGiRp",
    "outputId": "9bf07d1e-c297-44d2-da1f-80797018b6b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/ModelDebug/classifier\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# 加载预训练模型\n",
    "model_name = 'bert-base-uncased' # 预训练模型名字\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(model_name)\n",
    "model = transformers.BertForSequenceClassification.from_pretrained(model_name, num_labels=2) # 需要预测的类别数为2\n",
    "\n",
    "# 设置训练参数\n",
    "optimizer = transformers.AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# 加载数据集\n",
    "remote = True\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "if remote:\n",
    "  os.chdir('/content/drive/MyDrive/ModelDebug/classifier')\n",
    "  print(os.getcwd())\n",
    "  df = pd.read_csv('/content/drive/MyDrive/ModelDebug/classifier/textNFR.csv',usecols=['RequirementText','NFR'])\n",
    "  df1 = pd.read_csv('/content/drive/MyDrive/ModelDebug/classifier/new_dataset1.csv',usecols=['RequirementText','NFR'])\n",
    "  df2 = pd.read_csv('/content/drive/MyDrive/ModelDebug/classifier/new_dataset2.csv',usecols=['RequirementText','NFR'])\n",
    "  df3 = pd.read_csv('/content/drive/MyDrive/ModelDebug/classifier/new_dataset3.csv',usecols=['RequirementText','NFR'])\n",
    "  #dfenhence1 = pd.merge(df,newdf,on=['RequirementText','NFR'],how='outer')\n",
    "else :\n",
    "  os.chdir('../classifier')\n",
    "  df = pd.read_csv('textNFR.csv')\n",
    "train_data, test_data = train_test_split(df, test_size=0.2)\n",
    "#train_data, test_data = train_test_split(newdf, test_size=0.2)\n",
    "train_tokenized = tokenizer(\n",
    "    train_data[\"RequirementText\"].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    ")\n",
    "\n",
    "test_tokenized = tokenizer(\n",
    "    test_data[\"RequirementText\"].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    ")\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(train_tokenized[\"input_ids\"]),\n",
    "    torch.tensor(train_tokenized[\"attention_mask\"]),\n",
    "    torch.tensor(train_data[\"NFR\"].tolist()),\n",
    ")\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(test_tokenized[\"input_ids\"]),\n",
    "    torch.tensor(test_tokenized[\"attention_mask\"]),\n",
    "    torch.tensor(test_data[\"NFR\"].tolist()),\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeon-LrOJLdo"
   },
   "source": [
    "## 提前停止"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1682390412568,
     "user": {
      "displayName": "lu han",
      "userId": "00064302419450254877"
     },
     "user_tz": -480
    },
    "id": "HWTsrUyEJNS9"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Early stopping utility to stop training if no improvement after certain epochs.\n",
    "        \n",
    "        Args:\n",
    "        patience (int): How long to wait after last time validation loss improved. Default: 5.\n",
    "        verbose (bool): If True, prints a message for each validation loss improvement. Default: False.\n",
    "        delta (float): Minimum change in the monitored quantity to qualify as an improvement. Default: 0.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            self.val_loss_min = val_loss\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9kDUBIIAfBO"
   },
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tM_kHq4KD1tP"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, f1_score\n",
    "model.to(device)\n",
    "# 记录每个 epoch 的召回率和准确率\n",
    "train_recall_list = []\n",
    "test_recall_list = []\n",
    "test_accuracy_list = []\n",
    "test_f1_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # 计算训练集召回率\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            train_preds.extend(predicted.cpu().tolist())\n",
    "            train_labels.extend(labels.cpu().tolist())\n",
    "    train_recall = recall_score(train_labels, train_preds, average='macro')\n",
    "    train_recall_list.append(train_recall)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            test_loss += loss.item()\n",
    "            logits = outputs.logits\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        test_loss = test_loss / len(test_loader)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        # 计算测试集召回率和 F1 分数\n",
    "        test_preds = []\n",
    "        test_labels = []\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            test_preds.extend(predicted.cpu().tolist())\n",
    "            test_labels.extend(labels.cpu().tolist())\n",
    "        test_recall = recall_score(test_labels, test_preds, average='macro')\n",
    "        test_recall_list.append(test_recall)\n",
    "        test_accuracy_list.append(accuracy)\n",
    "        test_f1 = f1_score(test_labels, test_preds, average='macro')\n",
    "        test_f1_list.append(test_f1)\n",
    "    # early_stopping(test_loss)\n",
    "    # if early_stopping.early_stop:\n",
    "    #     print(\"Early stopping\")\n",
    "    #     break\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Test Accuracy: {accuracy:.4f} | Test Recall: {test_recall:.4f} | Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "# 输出平均性能参数\n",
    "print(f\"Average Test Accuracy: {sum(test_accuracy_list)/len(test_accuracy_list):.4f} | Average Test Recall: {sum(test_recall_list)/len(test_recall_list):.4f} | Average Test F1: {sum(test_f1_list)/len(test_f1_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8eVV9_enMbM"
   },
   "source": [
    "## baseline\n",
    "\n",
    "## debuged model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2148,
     "status": "ok",
     "timestamp": 1682392392541,
     "user": {
      "displayName": "lu han",
      "userId": "00064302419450254877"
     },
     "user_tz": -480
    },
    "id": "mItDKw8QlNcO"
   },
   "outputs": [],
   "source": [
    "# 保存微调后的模型\n",
    "if remote:\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #}, '/content/drive/MyDrive/transformers_ner_master/classifier/model.pt')\n",
    "    }, '/content/drive/MyDrive/ModelDebug/classifier/baseline.pt')\n",
    "else:\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, 'model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzSAr7v1Y51Y"
   },
   "source": [
    "# 模型优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1682392520436,
     "user": {
      "displayName": "lu han",
      "userId": "00064302419450254877"
     },
     "user_tz": -480
    },
    "id": "YLJ8_0ycY8Iu"
   },
   "outputs": [],
   "source": [
    "baseline = torch.load('./baseline.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahR8aJ0CaJvH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceXx3ghkjqqN"
   },
   "source": [
    "# 问题\n",
    "## 过拟合\n",
    "模型一开始，即第一个epoch就出现了过拟合，train loss=0.003,test loss=0.6.\n",
    "这说明数据集太少，可以通过我们的数据增强方法生成新的数据，来改善过拟合现象。\n",
    "# 实验数据\n",
    "## epoch=200,batchsize=4,no earlystopping,new dataset\n",
    "Average Test Accuracy: 0.8991 | Average Test Recall: 0.8999 | Average Test F1: 0.8978\n",
    "## epoch=200,batchsize=32,earlystopping new dataset\n",
    "Average Test Accuracy: 0.8207 | Average Test Recall: 0.8057 | Average Test F1: 0.8023\n",
    "## epoch=200,batchsize=32,earlystopping, old dataset\n",
    "Average Test Accuracy: 0.8967 | Average Test Recall: 0.8907 | Average Test F1: 0.8941\n",
    "## epoch=200,batchsize=32,earlystopping, old dataset+new dataset\n",
    "Average Test Accuracy: 0.9107 | Average Test Recall: 0.9063 | Average Test F1: 0.9080"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "",
   "version": ""
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
